{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1  机器学习数据集分析与处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "<class 'numpy.ndarray'>\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "(150, 4)\n",
      "目标值\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "--------------------------------------------------\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "--------------------------------------------------\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "--------------------------------------------------\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#鸢尾花数据集，查看特征，目标，样本量\n",
    "\n",
    "li = load_iris()\n",
    "\n",
    "print(\"获取特征值\")\n",
    "print(type(li.data))\n",
    "print(li.data)\n",
    "print(li.data.shape)\n",
    "print(\"目标值\")\n",
    "print(li.target)\n",
    "print('-' * 50)\n",
    "print(li.DESCR)\n",
    "print('-' * 50)\n",
    "print(li.feature_names)  # 重点\n",
    "print('-' * 50)\n",
    "print(li.target_names)\n",
    "print('-' * 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征值和目标值： [[6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.6 1.4 0.1]] [1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "训练集特征值shape (112, 4)\n",
      "测试集特征值和目标值： [[5.8 4.  1.2 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]] [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "测试集特征值shape (38, 4)\n"
     ]
    }
   ],
   "source": [
    "# 注意返回值, 训练集 train  x_train, y_train        测试集  test   x_test, y_test，顺序千万别搞错了\n",
    "# 默认是乱序的,random_state为了确保两次的随机策略一致，往往会带上\n",
    "x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25, random_state=1)\n",
    "\n",
    "print(\"训练集特征值和目标值：\", x_train, y_train)\n",
    "print(\"训练集特征值shape\", x_train.shape)\n",
    "\n",
    "print(\"测试集特征值和目标值：\", x_test, y_test)\n",
    "print(\"测试集特征值shape\", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      ".. dropdown:: Usage\n",
      "\n",
      "  The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "  fetching / caching functions that downloads the data archive from\n",
      "  the original `20 newsgroups website <http://people.csail.mit.edu/jrennie/20Newsgroups/>`__,\n",
      "  extracts the archive contents\n",
      "  in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      "  :func:`sklearn.datasets.load_files` on either the training or\n",
      "  testing set folder, or both of them::\n",
      "\n",
      "    >>> from sklearn.datasets import fetch_20newsgroups\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "    >>> from pprint import pprint\n",
      "    >>> pprint(list(newsgroups_train.target_names))\n",
      "    ['alt.atheism',\n",
      "     'comp.graphics',\n",
      "     'comp.os.ms-windows.misc',\n",
      "     'comp.sys.ibm.pc.hardware',\n",
      "     'comp.sys.mac.hardware',\n",
      "     'comp.windows.x',\n",
      "     'misc.forsale',\n",
      "     'rec.autos',\n",
      "     'rec.motorcycles',\n",
      "     'rec.sport.baseball',\n",
      "     'rec.sport.hockey',\n",
      "     'sci.crypt',\n",
      "     'sci.electronics',\n",
      "     'sci.med',\n",
      "     'sci.space',\n",
      "     'soc.religion.christian',\n",
      "     'talk.politics.guns',\n",
      "     'talk.politics.mideast',\n",
      "     'talk.politics.misc',\n",
      "     'talk.religion.misc']\n",
      "\n",
      "  The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "  attribute is the integer index of the category::\n",
      "\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "  It is possible to load only a sub-selection of the categories by passing the\n",
      "  list of the categories to load to the\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "    >>> cats = ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "    >>> list(newsgroups_train.target_names)\n",
      "    ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      ".. dropdown:: Converting text to vectors\n",
      "\n",
      "  In order to feed predictive or clustering models with the text data,\n",
      "  one first need to turn the text into vectors of numerical values suitable\n",
      "  for statistical analysis. This can be achieved with the utilities of the\n",
      "  ``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "  example that extract `TF-IDF <https://en.wikipedia.org/wiki/Tf-idf>`__ vectors\n",
      "  of unigram tokens from a subset of 20news::\n",
      "\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "    ...               'comp.graphics', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> vectors.shape\n",
      "    (2034, 34118)\n",
      "\n",
      "  The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "  components by sample in a more than 30000-dimensional space\n",
      "  (less than .5% non-zero features)::\n",
      "\n",
      "    >>> vectors.nnz / float(vectors.shape[0])\n",
      "    159.01327...\n",
      "\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "  returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. dropdown:: Filtering text for more realistic training\n",
      "\n",
      "  It is easy for a classifier to overfit on particular things that appear in the\n",
      "  20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "  high F-scores, but their results would not generalize to other documents that\n",
      "  aren't from this window of time.\n",
      "\n",
      "  For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "  which is fast to train and achieves a decent F-score::\n",
      "\n",
      "    >>> from sklearn.naive_bayes import MultinomialNB\n",
      "    >>> from sklearn import metrics\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.88213...\n",
      "\n",
      "  (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "  the training and test data, instead of segmenting by time, and in that case\n",
      "  multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "  yet of what's going on inside this classifier?)\n",
      "\n",
      "  Let's take a look at what the most informative features are:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def show_top10(classifier, vectorizer, categories):\n",
      "    ...     feature_names = vectorizer.get_feature_names_out()\n",
      "    ...     for i, category in enumerate(categories):\n",
      "    ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "    ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "    ...\n",
      "    >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "    alt.atheism: edu it and in you that is of to the\n",
      "    comp.graphics: edu in graphics it is for and of to the\n",
      "    sci.space: edu it that is in and space to of the\n",
      "    talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "  You can now see many things that these features have overfit to:\n",
      "\n",
      "  - Almost every group is distinguished by whether headers such as\n",
      "    ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "  - Another significant feature involves whether the sender is affiliated with\n",
      "    a university, as indicated either by their headers or their signature.\n",
      "  - The word \"article\" is a significant feature, based on how often people quote\n",
      "    previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "    wrote:\"\n",
      "  - Other features match the names and e-mail addresses of particular people who\n",
      "    were posting at the time.\n",
      "\n",
      "  With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "  barely have to identify topics from text at all, and they all perform at the\n",
      "  same high level.\n",
      "\n",
      "  For this reason, the functions that load 20 Newsgroups data provide a\n",
      "  parameter called **remove**, telling it what kinds of information to strip out\n",
      "  of each file. **remove** should be a tuple containing any subset of\n",
      "  ``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "  blocks, and quotation blocks respectively.\n",
      "\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "    0.77310...\n",
      "\n",
      "  This classifier lost over a lot of its F-score, just because we removed\n",
      "  metadata that has little to do with topic classification.\n",
      "  It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.76995...\n",
      "\n",
      "  Some other classifiers cope better with this harder version of the task. Try the\n",
      "  :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "  example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. rubric:: Examples\n",
      "\n",
      "* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n",
      "--------------------------------------------------\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "10\n",
      "18846\n",
      "--------------------------------------------------\n",
      "[10  3 17 ...  3  1  7]\n",
      "0 19\n"
     ]
    }
   ],
   "source": [
    "# 下面是比较大的数据，需要下载一会，20类新闻\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "# print(news.feature_names)  #这个没有\n",
    "print(news.DESCR)\n",
    "print('-' * 50)\n",
    "print(news.data[0])\n",
    "print(news.target[0])\n",
    "print(len(news.data))\n",
    "print('-' * 50)\n",
    "print(news.target)\n",
    "print(min(news.target), max(news.target))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 分类估计器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id       x       y  accuracy    time    place_id\n",
      "0       0  0.7941  9.0809        54  470702  8523065625\n",
      "1       1  5.9567  4.7968        13  186555  1757726713\n",
      "2       2  8.3078  7.0407        74  322648  1137537235\n",
      "3       3  7.3665  2.5165        65  704587  6567393236\n",
      "4       4  4.0961  1.1307        31  472130  7440663949\n",
      "5       5  3.8099  1.9586        75  178065  6289802927\n",
      "6       6  6.3336  4.3720        13  666829  9931249544\n",
      "7       7  5.7409  6.7697        85  369002  5662813655\n",
      "8       8  4.3114  6.9410         3  166384  8471780938\n",
      "9       9  6.3414  0.0758        65  400060  1253803156\n",
      "(29118021, 6)\n",
      "600        1970-01-01 18:09:40\n",
      "957        1970-01-10 02:11:10\n",
      "4345       1970-01-05 15:08:02\n",
      "4735       1970-01-06 23:03:03\n",
      "5580       1970-01-09 11:26:50\n",
      "                   ...        \n",
      "29100203   1970-01-01 10:33:56\n",
      "29108443   1970-01-07 23:22:04\n",
      "29109993   1970-01-08 15:03:14\n",
      "29111539   1970-01-04 00:53:41\n",
      "29112154   1970-01-08 23:01:07\n",
      "Name: time, Length: 17710, dtype: datetime64[ns]\n",
      "--------------------------------------------------\n",
      "DatetimeIndex(['1970-01-01 18:09:40', '1970-01-10 02:11:10',\n",
      "               '1970-01-05 15:08:02', '1970-01-06 23:03:03',\n",
      "               '1970-01-09 11:26:50', '1970-01-02 16:25:07',\n",
      "               '1970-01-04 15:52:57', '1970-01-01 10:13:36',\n",
      "               '1970-01-09 15:26:06', '1970-01-08 23:52:02',\n",
      "               ...\n",
      "               '1970-01-07 10:03:36', '1970-01-09 11:44:34',\n",
      "               '1970-01-04 08:07:44', '1970-01-04 15:47:47',\n",
      "               '1970-01-08 01:24:11', '1970-01-01 10:33:56',\n",
      "               '1970-01-07 23:22:04', '1970-01-08 15:03:14',\n",
      "               '1970-01-04 00:53:41', '1970-01-08 23:01:07'],\n",
      "              dtype='datetime64[ns]', name='time', length=17710, freq=None)\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "--------------------------------------------------\n",
      "            row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600            600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957            957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345          4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735          4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580          5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "...            ...     ...     ...       ...         ...  ...   ...      ...\n",
      "29100203  29100203  1.0129  2.6775        12  3312463746    1    10        3\n",
      "29108443  29108443  1.1474  2.6840        36  3533177779    7    23        2\n",
      "29109993  29109993  1.0240  2.7238        62  6424972551    8    15        3\n",
      "29111539  29111539  1.2032  2.6796        87  3533177779    4     0        6\n",
      "29112154  29112154  1.1070  2.5419       178  4932578245    8    23        3\n",
      "\n",
      "[17710 rows x 8 columns]\n",
      "            row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600            600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957            957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345          4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735          4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580          5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "...            ...     ...     ...       ...         ...  ...   ...      ...\n",
      "29100203  29100203  1.0129  2.6775        12  3312463746    1    10        3\n",
      "29108443  29108443  1.1474  2.6840        36  3533177779    7    23        2\n",
      "29109993  29109993  1.0240  2.7238        62  6424972551    8    15        3\n",
      "29111539  29111539  1.2032  2.6796        87  3533177779    4     0        6\n",
      "29112154  29112154  1.1070  2.5419       178  4932578245    8    23        3\n",
      "\n",
      "[16918 rows x 8 columns]\n",
      "(16918, 6)\n",
      "Index(['x', 'y', 'accuracy', 'day', 'hour', 'weekday'], dtype='object')\n",
      "[ 1.12295735  2.63237278 81.34938525  5.10064628 11.44293821  3.10135561]\n",
      "[5.98489138e-03 4.86857391e-03 1.19597480e+04 7.32837915e+00\n",
      " 4.83742660e+01 2.81838404e+00]\n",
      "--------------------------------------------------\n",
      "[ 1.12295735  2.63237278 81.34938525  5.10064628 11.44293821  3.10135561]\n",
      "[5.98489138e-03 4.86857391e-03 1.19597480e+04 7.32837915e+00\n",
      " 4.83742660e+01 2.81838404e+00]\n",
      "预测的目标签到位置为： [5689129232 1097200869 2355236719 ... 4932578245 6424972551 4492549925]\n",
      "预测的准确率: 0.4806146572104019\n"
     ]
    }
   ],
   "source": [
    "# K近邻\n",
    "\"\"\"\n",
    "K-近邻预测用户签到位置\n",
    ":return:None\n",
    "\"\"\"\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"./data/FBlocation/train.csv\")\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.shape)\n",
    "# 处理数据\n",
    "# 1、缩小数据,查询数据,为了减少计算时间\n",
    "data = data.query(\"x > 1.0 &  x < 1.25 & y > 2.5 & y < 2.75\")\n",
    "\n",
    "# 处理时间的数据\n",
    "time_value = pd.to_datetime(data['time'], unit='s')\n",
    "\n",
    "print(time_value)  #最大时间是1月10号\n",
    "#\n",
    "# 把日期格式转换成 字典格式，把年，月，日，时，分，秒转换为字典格式，\n",
    "time_value = pd.DatetimeIndex(time_value)\n",
    "#\n",
    "print('-' * 50)\n",
    "print(time_value)\n",
    "print('-' * 50)\n",
    "# 构造一些特征，执行的警告是因为我们的操作是复制，loc是直接放入\n",
    "print(type(data))\n",
    "# data['day'] = time_value.day\n",
    "# data['hour'] = time_value.hour\n",
    "# data['weekday'] = time_value.weekday\n",
    "#日期，是否是周末，小时对于个人行为的影响是较大的\n",
    "data.insert(data.shape[1], 'day', time_value.day)\n",
    "data.insert(data.shape[1], 'hour', time_value.hour)\n",
    "data.insert(data.shape[1], 'weekday', time_value.weekday)\n",
    "\n",
    "#\n",
    "# 把时间戳特征删除\n",
    "data = data.drop(['time'], axis=1)\n",
    "print('-' * 50)\n",
    "print(data)\n",
    "#\n",
    "# # 把签到数量少于n个目标位置删除\n",
    "place_count = data.groupby('place_id').count()\n",
    "# # 把index变为0,1,2，3,4,5,6这种效果，从零开始拍，原来的index是地点\n",
    "#只选择去的人大于3的数据，认为1,2,3的是噪音\n",
    "tf = place_count[place_count.row_id > 3].reset_index()\n",
    "# 根据设定的地点目标值，对原本的样本进行过滤\n",
    "data = data[data['place_id'].isin(tf.place_id)]\n",
    "print(data)\n",
    "# # 取出数据当中的特征值和目标值\n",
    "y = data['place_id']\n",
    "# 删除目标值，保留特征值，\n",
    "x = data.drop(['place_id'], axis=1)\n",
    "# 删除无用的特征值\n",
    "x = x.drop(['row_id'], axis=1)\n",
    "print(x.shape)\n",
    "print(x.columns)\n",
    "# 进行数据的分割训练集合测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# 特征工程（标准化）,下面3行注释，一开始我们不进行标准化，看下效果，目标值要不要标准化？\n",
    "std = StandardScaler()\n",
    "# #\n",
    "# # # 对测试集和训练集的特征值进行标准化,服务于knn fit\n",
    "x_train = std.fit_transform(x_train)\n",
    "# # transform返回的是copy，不在原有的输入对象中去修改\n",
    "# print(id(x_test))\n",
    "print(std.mean_)\n",
    "print(std.var_)\n",
    "x_test = std.transform(x_test)  #transfrom不再进行均值和方差的计算，是在原有的基础上去标准化\n",
    "print('-' * 50)\n",
    "# print(id(x_test))\n",
    "print(std.mean_)\n",
    "print(std.var_)\n",
    "# # 进行算法流程 # 超参数，可以通过设置n_neighbors=5，来调整结果好坏\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# # fit， predict,score，训练\n",
    "knn.fit(x_train, y_train)\n",
    "# # #\n",
    "# # # 得出预测结果\n",
    "y_predict = knn.predict(x_test)\n",
    "# #\n",
    "print(\"预测的目标签到位置为：\", y_predict)\n",
    "# # #\n",
    "# # # # 得出准确率\n",
    "print(\"预测的准确率:\", knn.score(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\30663\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上准确率： 0.47966903073286055\n",
      "在交叉验证当中最好的结果： 0.4596467484726366\n",
      "选择最好的模型是： KNeighborsClassifier(n_neighbors=10)\n",
      "每个超参数每次交叉验证的结果： {'mean_fit_time': array([0.01781034, 0.01286697, 0.0156974 , 0.01768335, 0.0198218 ]), 'std_fit_time': array([0.00181964, 0.00143605, 0.00219984, 0.00341915, 0.00239617]), 'mean_score_time': array([0.37284748, 0.37096977, 0.44089993, 0.44794035, 0.46779172]), 'std_score_time': array([0.00638957, 0.01350134, 0.02152746, 0.01600003, 0.0402869 ]), 'param_n_neighbors': masked_array(data=[3, 5, 10, 12, 15],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value=999999), 'params': [{'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 10}, {'n_neighbors': 12}, {'n_neighbors': 15}], 'split0_test_score': array([0.44468085, 0.4607565 , 0.46170213, 0.45650118, 0.45508274]), 'split1_test_score': array([0.43390873, 0.45660913, 0.45542681, 0.45329865, 0.44809648]), 'split2_test_score': array([0.43982029, 0.45684559, 0.4618113 , 0.45897375, 0.46062899]), 'mean_test_score': array([0.43946996, 0.45807041, 0.45964675, 0.45625786, 0.45460274]), 'std_test_score': array([0.00440467, 0.00190181, 0.00298428, 0.00232323, 0.00512762]), 'rank_test_score': array([5, 2, 1, 3, 4], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# # 构造一些参数的值进行搜索\n",
    "param = {\"n_neighbors\": [3, 5, 10, 12, 15]}\n",
    "#\n",
    "# 进行网格搜索，cv=3是3折交叉验证，用其中2折训练，1折验证\n",
    "gc = GridSearchCV(knn, param_grid=param, cv=3)\n",
    "\n",
    "gc.fit(x_train, y_train)  #你给它的x_train，它又分为训练集，验证集\n",
    "\n",
    "# 预测准确率，为了给大家看看\n",
    "print(\"在测试集上准确率：\", gc.score(x_test, y_test))\n",
    "\n",
    "print(\"在交叉验证当中最好的结果：\", gc.best_score_)\n",
    "\n",
    "print(\"选择最好的模型是：\", gc.best_estimator_)\n",
    "\n",
    "print(\"每个超参数每次交叉验证的结果：\", gc.cv_results_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "[10  3 17 ...  3  1  7]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "预测的文章类别为： [16 19 18 ... 13  7 14]\n",
      "准确率为： 0.8518675721561969\n",
      "每个类别的精确率和召回率：                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.91      0.77      0.83       199\n",
      "           comp.graphics       0.83      0.79      0.81       242\n",
      " comp.os.ms-windows.misc       0.89      0.83      0.86       263\n",
      "comp.sys.ibm.pc.hardware       0.80      0.83      0.81       262\n",
      "   comp.sys.mac.hardware       0.90      0.88      0.89       234\n",
      "          comp.windows.x       0.92      0.85      0.88       230\n",
      "            misc.forsale       0.96      0.67      0.79       257\n",
      "               rec.autos       0.90      0.87      0.88       265\n",
      "         rec.motorcycles       0.90      0.95      0.92       251\n",
      "      rec.sport.baseball       0.89      0.96      0.93       226\n",
      "        rec.sport.hockey       0.95      0.98      0.96       262\n",
      "               sci.crypt       0.76      0.97      0.85       257\n",
      "         sci.electronics       0.84      0.80      0.82       229\n",
      "                 sci.med       0.97      0.86      0.91       249\n",
      "               sci.space       0.92      0.96      0.94       256\n",
      "  soc.religion.christian       0.55      0.98      0.70       243\n",
      "      talk.politics.guns       0.76      0.96      0.85       234\n",
      "   talk.politics.mideast       0.93      0.99      0.96       224\n",
      "      talk.politics.misc       0.98      0.56      0.72       197\n",
      "      talk.religion.misc       0.97      0.26      0.41       132\n",
      "\n",
      "                accuracy                           0.85      4712\n",
      "               macro avg       0.88      0.84      0.84      4712\n",
      "            weighted avg       0.87      0.85      0.85      4712\n",
      "\n",
      "AUC指标： 0.8827602448315142\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "朴素贝叶斯进行文本分类\n",
    ":return: None\n",
    "\"\"\"\n",
    "news = fetch_20newsgroups(subset='all', data_home='data')\n",
    "\n",
    "print(len(news.data))\n",
    "print(news.target)\n",
    "print(news.target_names)\n",
    "# 进行数据分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# 对数据集进行特征抽取\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "x_train = tf.fit_transform(x_train)\n",
    "\n",
    "# print(tf.get_feature_names())\n",
    "\n",
    "x_test = tf.transform(x_test)\n",
    "\n",
    "# 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "\n",
    "print(x_train.toarray())\n",
    "# 训练\n",
    "mlt.fit(x_train, y_train)\n",
    "\n",
    "y_predict = mlt.predict(x_test)\n",
    "\n",
    "print(\"预测的文章类别为：\", y_predict)\n",
    "\n",
    "# 得出准确率,这个是很难提高准确率，为什么呢？\n",
    "print(\"准确率为：\", mlt.score(x_test, y_test))\n",
    "# 目前这个场景我们不需要召回率，support是划分为那个类别的有多少个样本\n",
    "print(\"每个类别的精确率和召回率：\", classification_report(y_test, y_predict, target_names=news.target_names))\n",
    "# 把0-19总计20个分类，变为0和1\n",
    "y_test = np.where(y_test == 0, 1, 0)\n",
    "y_predict = np.where(y_predict == 0, 1, 0)\n",
    "# roc_auc_score的y_test只能是二分类,针对多分类如何计算AUC\n",
    "print(\"AUC指标：\", roc_auc_score(y_test, y_predict))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 决策树算法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pclass      age     sex\n",
      "0       1st  29.0000  female\n",
      "1       1st   2.0000  female\n",
      "2       1st  30.0000    male\n",
      "3       1st  25.0000  female\n",
      "4       1st   0.9167    male\n",
      "...     ...      ...     ...\n",
      "1308    3rd      NaN    male\n",
      "1309    3rd      NaN    male\n",
      "1310    3rd      NaN    male\n",
      "1311    3rd      NaN  female\n",
      "1312    3rd      NaN    male\n",
      "\n",
      "[1313 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    决策树对泰坦尼克号进行预测生死\n",
    "    :return: None\n",
    " \"\"\"\n",
    "# 获取数据\n",
    "titan = pd.read_csv(\"./data/titanic.txt\")\n",
    "\n",
    "# 处理数据，找出特征值和目标值\n",
    "x = titan[['pclass', 'age', 'sex']]\n",
    "\n",
    "y = titan['survived']\n",
    "\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pclass        age     sex\n",
      "1161    3rd  31.194181  female\n",
      "882     3rd  31.194181    male\n",
      "643     3rd   9.000000    male\n",
      "1112    3rd  31.194181  female\n",
      "454     2nd  33.000000    male\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\30663\\AppData\\Local\\Temp\\ipykernel_20064\\382114230.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  x['age'].fillna(x['age'].mean(), inplace=True)\n",
      "C:\\Users\\30663\\AppData\\Local\\Temp\\ipykernel_20064\\382114230.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['age'].fillna(x['age'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "    # 一定要进行缺失值处理\n",
    "x['age'].fillna(x['age'].mean(), inplace=True)\n",
    "\n",
    "# 分割数据集到训练集合测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "print(x_train.head())\n",
    "print(type(x_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['age' 'pclass=1st' 'pclass=2nd' 'pclass=3rd' 'sex=female' 'sex=male']\n",
      "--------------------------------------------------\n",
      "[[31.19418104  0.          0.          1.          1.          0.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [ 9.          0.          0.          1.          0.          1.        ]\n",
      " ...\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]\n",
      " [31.19418104  0.          0.          1.          0.          1.        ]]\n",
      "预测的准确率： 0.7781155015197568\n"
     ]
    }
   ],
   "source": [
    " # 进行处理（特征工程）特征-》类别-》one_hot编码\n",
    "dict = DictVectorizer(sparse=False)\n",
    "# 这一步是对字典进行特征抽取\n",
    "x_train = dict.fit_transform(x_train.to_dict(orient=\"records\"))\n",
    "print(type(x_train))\n",
    "print(dict.get_feature_names_out())\n",
    "print('-' * 50)\n",
    "x_test = dict.transform(x_test.to_dict(orient=\"records\"))\n",
    "print(x_train)\n",
    "#用决策树进行预测，常用的剪支，预剪枝：max_depth=10；\n",
    "dec = DecisionTreeClassifier()\n",
    "\n",
    "dec.fit(x_train, y_train)\n",
    "\n",
    "# 预测准确率\n",
    "print(\"预测的准确率：\", dec.score(x_test, y_test))\n",
    "#\n",
    "# 导出决策树的结构\n",
    "export_graphviz(dec, out_file=\"tree.dot\",\n",
    "                feature_names=dict.get_feature_names_out())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 集成学习-随机森林"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.790273556231003\n",
      "查看选择的参数模型： {'max_depth': 2, 'n_estimators': 120}\n",
      "选择最好的模型是： RandomForestClassifier(max_depth=2, n_estimators=120, n_jobs=-1)\n",
      "每个超参数每次交叉验证的结果： {'mean_fit_time': array([0.23462009, 0.32573811, 0.52925952, 0.8232131 , 1.30360818,\n",
      "       2.16683531, 3.06915975, 0.2339623 , 0.33333739, 0.5787696 ,\n",
      "       0.77096446, 1.30754519, 2.05240687, 3.2641294 , 0.23627083,\n",
      "       0.32073148, 0.52792708, 0.76310023, 1.35351133, 2.03745675,\n",
      "       3.16971024, 0.22113593, 0.34194946, 0.5526545 , 0.77179853,\n",
      "       1.2895751 , 2.09081753, 3.06875006, 0.23020379, 0.35202273,\n",
      "       0.55289777, 0.82940658, 1.38189618, 2.15912366, 3.22790241,\n",
      "       0.24459314, 0.32238913, 0.51971722, 0.76330002, 1.30520368,\n",
      "       2.06308643, 3.05652873]), 'std_fit_time': array([0.00422218, 0.00787127, 0.02022487, 0.11506574, 0.04700994,\n",
      "       0.08622726, 0.04878906, 0.00197278, 0.00900049, 0.05206972,\n",
      "       0.00264836, 0.05462645, 0.02412847, 0.1064097 , 0.00737666,\n",
      "       0.00696235, 0.00786297, 0.00214635, 0.09089339, 0.01706888,\n",
      "       0.11953304, 0.0006684 , 0.01240324, 0.03749347, 0.00204576,\n",
      "       0.01276663, 0.02743116, 0.03832695, 0.01431405, 0.00777827,\n",
      "       0.01283708, 0.03690539, 0.01410222, 0.07777346, 0.18434399,\n",
      "       0.03830358, 0.01054314, 0.00218855, 0.00770533, 0.08277086,\n",
      "       0.04856785, 0.02733044]), 'mean_score_time': array([0.04499722, 0.05366747, 0.06977185, 0.09752488, 0.1821177 ,\n",
      "       0.25424822, 0.37299641, 0.03945486, 0.05567829, 0.07534432,\n",
      "       0.11315362, 0.16230647, 0.25603008, 0.37922986, 0.04879991,\n",
      "       0.05777351, 0.07686885, 0.10837181, 0.1697375 , 0.2447296 ,\n",
      "       0.35858409, 0.04159021, 0.05947526, 0.08394122, 0.1042343 ,\n",
      "       0.16691009, 0.25682465, 0.35752877, 0.04434776, 0.05773274,\n",
      "       0.08374286, 0.10674198, 0.17251635, 0.26701681, 0.38299942,\n",
      "       0.04611444, 0.05242904, 0.07221746, 0.10386968, 0.16801834,\n",
      "       0.2460707 , 0.35886359]), 'std_score_time': array([0.0104669 , 0.00461737, 0.00376807, 0.0045934 , 0.016183  ,\n",
      "       0.01066092, 0.01940737, 0.00284844, 0.00332497, 0.00261424,\n",
      "       0.00895011, 0.0069764 , 0.00859314, 0.00610084, 0.00461286,\n",
      "       0.00607049, 0.00644026, 0.00452099, 0.01780818, 0.00477418,\n",
      "       0.00975697, 0.00209772, 0.00948319, 0.01145619, 0.00087765,\n",
      "       0.00864997, 0.00649941, 0.00621544, 0.00511391, 0.0069178 ,\n",
      "       0.00144469, 0.01105952, 0.00862142, 0.00151134, 0.02040618,\n",
      "       0.00578659, 0.00166373, 0.00180553, 0.0014521 , 0.01539091,\n",
      "       0.00580103, 0.00517885]), 'param_max_depth': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
      "                   5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 15, 15, 15, 15, 15, 15,\n",
      "                   15, 25, 25, 25, 25, 25, 25, 25],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value=999999), 'param_n_estimators': masked_array(data=[80, 120, 200, 300, 500, 800, 1200, 80, 120, 200, 300,\n",
      "                   500, 800, 1200, 80, 120, 200, 300, 500, 800, 1200, 80,\n",
      "                   120, 200, 300, 500, 800, 1200, 80, 120, 200, 300, 500,\n",
      "                   800, 1200, 80, 120, 200, 300, 500, 800, 1200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value=999999), 'params': [{'max_depth': 2, 'n_estimators': 80}, {'max_depth': 2, 'n_estimators': 120}, {'max_depth': 2, 'n_estimators': 200}, {'max_depth': 2, 'n_estimators': 300}, {'max_depth': 2, 'n_estimators': 500}, {'max_depth': 2, 'n_estimators': 800}, {'max_depth': 2, 'n_estimators': 1200}, {'max_depth': 3, 'n_estimators': 80}, {'max_depth': 3, 'n_estimators': 120}, {'max_depth': 3, 'n_estimators': 200}, {'max_depth': 3, 'n_estimators': 300}, {'max_depth': 3, 'n_estimators': 500}, {'max_depth': 3, 'n_estimators': 800}, {'max_depth': 3, 'n_estimators': 1200}, {'max_depth': 5, 'n_estimators': 80}, {'max_depth': 5, 'n_estimators': 120}, {'max_depth': 5, 'n_estimators': 200}, {'max_depth': 5, 'n_estimators': 300}, {'max_depth': 5, 'n_estimators': 500}, {'max_depth': 5, 'n_estimators': 800}, {'max_depth': 5, 'n_estimators': 1200}, {'max_depth': 8, 'n_estimators': 80}, {'max_depth': 8, 'n_estimators': 120}, {'max_depth': 8, 'n_estimators': 200}, {'max_depth': 8, 'n_estimators': 300}, {'max_depth': 8, 'n_estimators': 500}, {'max_depth': 8, 'n_estimators': 800}, {'max_depth': 8, 'n_estimators': 1200}, {'max_depth': 15, 'n_estimators': 80}, {'max_depth': 15, 'n_estimators': 120}, {'max_depth': 15, 'n_estimators': 200}, {'max_depth': 15, 'n_estimators': 300}, {'max_depth': 15, 'n_estimators': 500}, {'max_depth': 15, 'n_estimators': 800}, {'max_depth': 15, 'n_estimators': 1200}, {'max_depth': 25, 'n_estimators': 80}, {'max_depth': 25, 'n_estimators': 120}, {'max_depth': 25, 'n_estimators': 200}, {'max_depth': 25, 'n_estimators': 300}, {'max_depth': 25, 'n_estimators': 500}, {'max_depth': 25, 'n_estimators': 800}, {'max_depth': 25, 'n_estimators': 1200}], 'split0_test_score': array([0.83841463, 0.83841463, 0.83536585, 0.83841463, 0.84146341,\n",
      "       0.83841463, 0.83841463, 0.8445122 , 0.8445122 , 0.8445122 ,\n",
      "       0.8445122 , 0.8445122 , 0.8445122 , 0.8445122 , 0.82621951,\n",
      "       0.82621951, 0.82621951, 0.83231707, 0.82926829, 0.82926829,\n",
      "       0.82926829, 0.82012195, 0.81402439, 0.81707317, 0.81707317,\n",
      "       0.81707317, 0.81707317, 0.81707317, 0.81402439, 0.81097561,\n",
      "       0.82012195, 0.81402439, 0.81097561, 0.81097561, 0.81097561,\n",
      "       0.81097561, 0.81097561, 0.81097561, 0.81097561, 0.81097561,\n",
      "       0.81097561, 0.81097561]), 'split1_test_score': array([0.79878049, 0.81402439, 0.81097561, 0.81097561, 0.81097561,\n",
      "       0.81097561, 0.81097561, 0.81097561, 0.81097561, 0.81097561,\n",
      "       0.81097561, 0.81097561, 0.81097561, 0.81097561, 0.81707317,\n",
      "       0.81097561, 0.81402439, 0.81707317, 0.81707317, 0.82012195,\n",
      "       0.81707317, 0.82012195, 0.81402439, 0.82012195, 0.82012195,\n",
      "       0.82012195, 0.82012195, 0.82012195, 0.80487805, 0.81097561,\n",
      "       0.81097561, 0.81097561, 0.81097561, 0.81097561, 0.81097561,\n",
      "       0.81097561, 0.80487805, 0.81097561, 0.81097561, 0.81097561,\n",
      "       0.81097561, 0.81097561]), 'split2_test_score': array([0.81097561, 0.82012195, 0.82012195, 0.81707317, 0.82012195,\n",
      "       0.82012195, 0.82012195, 0.81402439, 0.81402439, 0.81402439,\n",
      "       0.81402439, 0.81402439, 0.81402439, 0.81402439, 0.81402439,\n",
      "       0.81402439, 0.80792683, 0.81402439, 0.81402439, 0.81402439,\n",
      "       0.81402439, 0.78963415, 0.81097561, 0.79268293, 0.79573171,\n",
      "       0.79268293, 0.80182927, 0.79573171, 0.80182927, 0.78963415,\n",
      "       0.79268293, 0.78658537, 0.78963415, 0.78963415, 0.78353659,\n",
      "       0.78963415, 0.78353659, 0.78658537, 0.78963415, 0.78353659,\n",
      "       0.79573171, 0.78963415]), 'mean_test_score': array([0.81605691, 0.82418699, 0.82215447, 0.82215447, 0.82418699,\n",
      "       0.82317073, 0.82317073, 0.82317073, 0.82317073, 0.82317073,\n",
      "       0.82317073, 0.82317073, 0.82317073, 0.82317073, 0.81910569,\n",
      "       0.81707317, 0.81605691, 0.82113821, 0.82012195, 0.82113821,\n",
      "       0.82012195, 0.80995935, 0.81300813, 0.80995935, 0.81097561,\n",
      "       0.80995935, 0.81300813, 0.81097561, 0.80691057, 0.80386179,\n",
      "       0.80792683, 0.80386179, 0.80386179, 0.80386179, 0.80182927,\n",
      "       0.80386179, 0.79979675, 0.80284553, 0.80386179, 0.80182927,\n",
      "       0.80589431, 0.80386179]), 'std_test_score': array([0.0165747 , 0.01036386, 0.01006046, 0.01176406, 0.01277419,\n",
      "       0.01140749, 0.01140749, 0.01514194, 0.01514194, 0.01514194,\n",
      "       0.01514194, 0.01514194, 0.01514194, 0.01514194, 0.00518193,\n",
      "       0.00658612, 0.00760499, 0.00800204, 0.00658612, 0.00626465,\n",
      "       0.00658612, 0.01437209, 0.00143721, 0.01227952, 0.01085069,\n",
      "       0.01227952, 0.00800204, 0.01085069, 0.00518193, 0.01006046,\n",
      "       0.01140749, 0.01227952, 0.01006046, 0.01006046, 0.01293488,\n",
      "       0.01006046, 0.01176406, 0.01149767, 0.01006046, 0.01293488,\n",
      "       0.00718604, 0.01006046]), 'rank_test_score': array([20,  1, 12, 13,  1,  3,  3,  3,  3,  3,  3,  3,  3,  3, 18, 19, 20,\n",
      "       14, 16, 14, 16, 26, 22, 26, 24, 26, 22, 24, 30, 32, 29, 32, 32, 32,\n",
      "       40, 32, 42, 39, 32, 40, 31, 32], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# 随机森林进行预测（超参数调优）\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "param = {\"n_estimators\": [80, 120, 200, 300, 500, 800, 1200], \"max_depth\": [2, 3, 5, 8, 15, 25]}\n",
    "\n",
    "# 网格搜索与交叉验证\n",
    "gc = GridSearchCV(rf, param_grid=param, cv=3)\n",
    "\n",
    "gc.fit(x_train, y_train)\n",
    "\n",
    "print(\"准确率：\", gc.score(x_test, y_test))\n",
    "\n",
    "print(\"查看选择的参数模型：\", gc.best_params_)\n",
    "\n",
    "print(\"选择最好的模型是：\", gc.best_estimator_)\n",
    "\n",
    "print(\"每个超参数每次交叉验证的结果：\", gc.cv_results_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3e717a92",
   "language": "python",
   "display_name": "PyCharm (day58)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}